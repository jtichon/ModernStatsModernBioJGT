---
title: "Class-5-Summary"
author: "Jenna G. Tichon"
date: "16/11/2019"
output:
  pdf_document:
    includes:
      in_header: header.tex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(mosaicsExample)
library(mosaics)
library(HistData)
library(bootstrap)
library(ggbeeswarm)
library(mixtools)
library(phyloseq)
```

# 7.2 What are the data? Matrices and their motivation

```{r}
turtles = read.table(here("data", "PaintedTurtles.txt"), header = TRUE)
turtles[1:4, ]
```

```{r}
load(here("data", "athletes.RData"))
athletes[1:3,]
```

```{r}
load(here("data", "Msig3transp.RData"))
round(Msig3transp,2)[1:5, 1:6]
```

```{r}
data("GlobalPatterns", package = "phyloseq")
GPOTUs = as.matrix(t(phyloseq::otu_table(GlobalPatterns)))
GPOTUs[1:4, 6:13]
```
```{r}
library("SummarizedExperiment")
data("airway", package = "airway")
assay(airway)[1:3, 1:4]
```
```{r}
metab = t(as.matrix(read.csv(here("data","metabolites.csv"), row.names = 1)))
metab[1:4, 1:4]
```
## Task

```{r}
length(which(metab==0))
```

```{r}
cor(turtles[,2:4])
```
## Q 7.3

(a)
```{r}
plot(turtles[,2:4])
library("GGally")
ggpairs(turtles[, -1], axisLabels = "none")
```

```{r}
ggpairs(athletes, axisLabels = "none")
```

```{r}
library("pheatmap")
pheatmap(cor(athletes), cell.width = 10, cell.height = 10)
```

## 7.2.2 Preprocessing the data

scale() in R makes everything in the matrix have mean 0 and standard deviation 1.

### Q 7.5
Compute means and standard deviations of turtle them use scale function to create \texttt{scaledTurtles}.

(a)
```{r}
#Find means and standard deviations
(turtles.mean<-turtles[,2:4] %>%
  map_dfr(mean) )
(turtles.sd<-turtles[,2:4] %>%
  map_dfr(sd) )
#Use scale functin
scaledTurtles<-scale(turtles[,2:4])
#Verify it now has mean 0 and standard deviation 1
apply(scaledTurtles, 2, sd)
apply(scaledTurtles, 2, mean)
ggplot(data = as.data.frame(scaledTurtles), mapping = aes(x = width, y = height, col = turtles$sex)) +
  geom_point()
```

Text code: note the simpler use of piping and combining scaledTurtles with the sex column before making the ggplot to make this simpler:

```{r}
data.frame(scaledTurtles, sex = turtles[, 1]) %>%
  ggplot(aes(x = width, y = height, group = sex)) +
    geom_point(aes(color = sex)) + coord_fixed()
```

# 7.3 Dimension reduction

Orthogonal projection of the athletes data onto the vector at y=0

```{r}
athletes = data.frame(scale(athletes))
#Produce a scatterplot of disc vs weight
ath_gg = ggplot(athletes, aes(x = weight, y = disc)) +
  geom_point(size = 2, shape = 21)
#Produce a straight line at y=0 in red then create dashed line from points to line. xend and yend say where to start and end the vertical lines in geom_segment
ath_gg + geom_point(aes(y = 0), colour = "red") +
  geom_segment(aes(xend = weight, yend = 0), linetype = "dashed")
```

(a) Calculate the variance of the red points
```{r}
var(athletes$weight)
```
this has a variance of 1 because these are scaled points with sd 1.

(b) Make a plot showing projection lines onto the $y$ axis and projected points
```{r}
#Produce a scatterplot of disc vs weight
ath_gg = ggplot(athletes, aes(x = weight, y = disc)) +
  geom_point(size = 2, shape = 21)
#Produce a straight line at x=0 in red then create dashed line from points to line
ath_gg + geom_point(aes(x = 0), colour = "red") +
  geom_segment(aes(xend = 0, yend = disc), linetype = "dashed")
```


(c) Compute the variance of the points projected onto the vertical $y$ axis
```{r}
var(athletes$disc)
```

## 7.3.2 How do we summarize two-dimensional data by a line?

Regress disc on weight
```{r}
#Regress disc on weight and extract the coefficients
reg1 = lm(disc ~ weight, data = athletes)
a1 = reg1$coefficients[1] # intercept
b1 = reg1$coefficients[2] # slope
#Fit the regression line to the scatterplot in blue
pline1 = ath_gg + geom_abline(intercept = a1, slope = b1,
    col = "blue", lwd = 1.5)
#Make arrows from points to fitted line
pline1 + geom_segment(aes(xend = weight, yend = reg1$fitted),
    colour = "red", arrow = arrow(length = unit(0.15, "cm")))
```

Redo flipping x and y
```{r}
reg2 = lm(weight ~ disc, data = athletes)
a2 = reg2$coefficients[1] # intercept
b2 = reg2$coefficients[2] # slope
pline2 = ath_gg + geom_abline(intercept = -a2/b2, slope = 1/b2,
    col = "darkgreen", lwd = 1.5)
pline2 + geom_segment(aes(xend=reg2$fitted, yend=disc),
    colour = "orange", arrow = arrow(length = unit(0.15, "cm")))
```

\alert{Look up single value decomposition}
Make the line that minimizes the orthogonal distances called the **principal component** line. Here are three ways of fitting it:

```{r}
xy = cbind(athletes$disc, athletes$weight)
#Calculate singular value decomposition (svd) of matrix with just x,y
svda = svd(xy)
#Calculate principal components
pc = xy %*% svda$v[, 1] %*% t(svda$v[, 1])
#slope of line
bp = svda$v[2, 1] / svda$v[1, 1]
#intercept of line
ap = mean(pc[, 2]) - bp * mean(pc[, 1])
#ath_gg already has scatterplot. geom_segment says to take each point and end at the xy principal component which falls on the pc line
ath_gg + geom_segment(xend = pc[, 1], yend = pc[, 2]) +
  geom_abline(intercept = ap, slope = bp, col = "purple", lwd = 1.5)
```
Note this passes through the origin as we centered all data and is in the middle of both regression lines

### Q 7.8

```{r}
var(athletes$disc*bp+ap)
```

Principal components is minimizing the orthogonal projections onto the line. It also maximized the variance of projections along the line. We want to think of liking maximizing variance as being able to "see" as much of the data as possible. 

## 7.4.1 Optimal Lines

# 7.6 The inner workings of PCA: rank reduction

## 7.6.1 Rank-one matrices

Looking for $u$ and $v$ such that $u*v^t$=matrix. This is not unique so pick vectors where sums of squares sum to 1. Then there is a scaling factor to get the "non-decimal" values back.

```{r}
#Square Matrix
X = matrix(c(780,  75, 540,
             936,  90, 648,
            1300, 125, 900,
             728,  70, 504), nrow = 3)
#u and v decomposition
u = c(0.8196, 0.0788, 0.5674)
v = c(0.4053, 0.4863, 0.6754, 0.3782)
#scaling factor
s1 = 2348.2
#Show sums of squares equal 1
sum(u^2)
sum(v^2)
#Get original matrix back from decomposition
s1 * u %*% t(v)
X - s1 * u %*% t(v)
```

```{r}
#The scaling factor is the first observation in the d, which matches the first columns of u and v which are the ones we used.
svd(X)
```

## 7.6.2 How do we find such a decomposition in a unique way?

```{r}
(Xtwo = matrix(c(12.5, 35.0, 25.0, 25, 9, 14, 26, 18, 16, 21, 49, 32,
       18, 28, 52, 36, 18, 10.5, 64.5, 36), ncol = 4, byrow = TRUE))
(USV = svd(Xtwo))
```
```{r}
#How far is approximation with first singular vectors from Xtwo
Xtwo - USV$d[1] * USV$u[, 1] %*% t(USV$v[, 1])
#How far off is second approximation from the first approximation
Xtwo - USV$d[1] * USV$u[, 1] %*% t(USV$v[, 1]) -
       USV$d[2] * USV$u[, 2] %*% t(USV$v[, 2])
#Little improvement from third
Xtwo - USV$d[1] * USV$u[, 1] %*% t(USV$v[, 1]) -
       USV$d[2] * USV$u[, 2] %*% t(USV$v[, 2]) -
       USV$d[3] * USV$u[, 3] %*% t(USV$v[, 3])
#Little from fourth
Xtwo - USV$d[1] * USV$u[, 1] %*% t(USV$v[, 1]) -
       USV$d[2] * USV$u[, 2] %*% t(USV$v[, 2]) -
       USV$d[3] * USV$u[, 3] %*% t(USV$v[, 3]) -
       USV$d[4] * USV$u[, 4] %*% t(USV$v[, 4])
```
Only second approximation improves so it is of rank 2. We'll also notice that second d from svd decomp was last "non-zero" term.

SVD for the turtles data:

```{r}
turtles.svd = svd(scaledTurtles)
turtles.svd$d
turtles.svd$v
dim(turtles.svd$u)
```

Note: co-efficients are mostly equal for all three axes. 

```{r}
#Sum of squares for v is 1
sum(turtles.svd$v[,1]^2)
# sum of the d's squares divided by n-1 (48-1) equals 3, number of columns (p)
sum(turtles.svd$d^2) / 47
#if coeff^2/(n-1)=p this mean (n-1)p = sum of soefficients squared
```

## 7.6.3 Singular value decomposition

TAke all of the $u$'s and add them to a matrix $U$ and all of the $v$'s and add them a matrix $V$. 

\[\mathbf{X} = USV^t, V^tV=\mathbb{I}, U^TU=\mathbb{I}\]

$S$ is the diagonal matrix of the singular values.

\[X_{ij}=u_{i1}s_1v_{1j}+u_{i2}s_2v_{2j}+\ldots+u_{ir}s_rv_{rj}\]

## Principal Components
We can create more informative variables by taking the singular value decomposiont coefficients in front of the original variables:

\[Z_1 = c_1\mathbf{X}_{.1}+c_2\mathbf{X}_{.2}+\dots+c_p\mathbf{X}_{.p}\]

These c's are the $(c_1,c_2,\dots)$ from the usv\$v of svd(X). They have decreasing variance

### Q 7.18 
WRite principal components of turtles data two ways:

```{r}
turtles.svd$d[1] %*% turtles.svd$u[,1]
scaledTurtles %*% turtles.svd$v[,1]
```

# 7.7 Plotting the observation in the principal plane

```{r}
svda
```
Looking at the $v$ entry from the athletes SVD, we have the first principal component as:
\[Z_1=-0.707*\text{athletes}disc-0.707*atheletes\text{weight}\]

### Q 7.19

Rotate our graph so that the purple pca line is horizontal to $x$-axis we get the first **principal plane**
```{r}
#Find the two Z lines from rescaling using the svd
ppdf = tibble(PC1n = -svda$u[, 1] * svda$d[1],
              PC2n = svda$u[, 2] * svda$d[2])
#Plot those values, add points for x of pc with y=o, add points for y of pc with x=0
ggplot(ppdf, aes(x = PC1n, y = PC2n)) + geom_point() + xlab("PC1 ")+
    ylab("PC2") + geom_point(aes(x=PC1n,y=0),color="red") +
#Add segments of points to both rescaled 0 axes    
    geom_segment(aes(xend = PC1n, yend = 0), color = "red") +
    geom_hline(yintercept = 0, color = "purple", lwd=1.5, alpha=0.5) +
    xlim(-3.5, 2.7) + ylim(-2,2) + coord_fixed()
segm = tibble(xmin = pmin(ppdf$PC1n, 0), xmax = pmax(ppdf$PC1n, 0), yp = seq(-1, -2, length = nrow(ppdf)), yo = ppdf$PC2n)
ggplot(ppdf, aes(x = PC1n, y = PC2n)) + geom_point() + ylab("PC2") + xlab("PC1") +
    geom_hline(yintercept=0,color="purple",lwd=1.5,alpha=0.5) +
    geom_point(aes(x=PC1n,y=0),color="red")+
    xlim(-3.5, 2.7)+ylim(-2,2)+coord_fixed() +
    geom_segment(aes(xend=PC1n,yend=0), color="red")+
    geom_segment(data=segm,aes(x=xmin,xend=xmax,y=yo,yend=yo), color="blue",alpha=0.5)
```

  Note: sum of squares of red segments is square of second singular value. variance of red lines is larger than blue as its the first component. ratio of standard deviation of red to blue is ratio of the singular values
  
```{r}
sd(ppdf$PC1n)/sd(ppdf$PC2n)
svda$d[1]/svda$d[2]
```

Using prcomp:

```{r}
prcomp(athletes[,1:2])
svda$v
```
  
## 7.7.1 PCA of the turtles data

```{r}
cor(scaledTurtles)
pcaturtles = princomp(scaledTurtles)
pcaturtles
```

```{r}
library("factoextra")
fviz_eig(pcaturtles, geom = "bar", bar_width = 0.4) + ggtitle("")
```
This shows that one principal components captures most of the variability of the data.

### Q 7.21

Three different pca functions developed at various points in time:

```{r}
library(ade4)
svd(scaledTurtles)$v[, 1]
prcomp(turtles[, -1])$rotation[, 1]
princomp(scaledTurtles)$loadings[, 1]
dudi.pca(turtles[, -1], nf = 2, scannf = FALSE)$c1[, 1]
```

Without scaling in princomp:
```{r}
princomp(turtles[,-1])$loadings[, 1]
```
We see that without scaling first, all of the values in the principle components are different but equal with scaling.

### Q 7.22

```{r}
(res = princomp(scaledTurtles))
(PC1 = scaledTurtles %*% res$loadings[,1])
(sd1 = sqrt(mean(res$scores[, 1]^2)))
(res$scores)
```

### Q 7.23

```{r}
fviz_pca_biplot(pcaturtles, label = "var", habillage = turtles[, 1]) +
  ggtitle("")
```

### Q 7.24

First component ("x") always has largest variance so the pca plot is always wider than it is tall ("y")

### Q 7.25

Females tend to be taller. Width and length are highly correlatd with one another as their axes are very similar

### Q 7.26

COmpare the variance of each new coordinate to the eigenvalues returned by the PCA dudi.pca function

```{r}
pcadudit = dudi.pca(scaledTurtles, nf = 2, scannf = FALSE)
apply(pcadudit$li, 2, function(x) sum(x^2)/48)
pcadudit$eig
```
The sums of squares of each new coordiate are similar to the eigenvalues.


REmember:

  - Each principal component has variance measured by the eigenvalue which is the square of the singular value
  - New variable are always orthogonal, centered, uncorrelated, hence independent
  - When scaled, sum of the variances equals number of variables. (sum of variances is sum of diagonal of cross product matrix, i.e. eigenvalues are the diagonal of the cross product matrix)
  - principal components are ordered by variances (i.e. eignenvalues)
  
## 7.7.2 A complete analysis: the decathlon athletes:

```{r}
cor(athletes) %>% round(1)
```

```{r}
#Scree plot to choose k
pca.ath = dudi.pca(athletes, scannf = FALSE)
pca.ath$eig
fviz_eig(pca.ath, geom = "bar", bar_width = 0.3) + ggtitle("")
```
Note this drops off around 2 so that's likely a good number of components

```{r}
fviz_pca_var(pca.ath, col.circle = "black") + ggtitle("")
```
Variables projected on two new axes. Space between denotes correlation. The opposite correlation is because some are running and some are throwing/jumping. The best athletes throw/jump the best (which is high) but run the best (which is low times) 

### Q 7.28 

Change the signs on the running events to get a positive correlation:

```{r}
athletes[, c(1, 5, 6, 10)] = -athletes[, c(1, 5, 6, 10)]
cor(athletes) %>% round(1)
pcan.ath = dudi.pca(athletes, nf = 2, scannf = FALSE)
pcan.ath$eig
fviz_pca_var(pcan.ath, col.circle="black") + ggtitle("")
```
Plot athletes projected onto first principal plane:

```{r}
fviz_pca_ind(pcan.ath) + ggtitle("") + ylim(c(-2.5,5.7))
```
The best athletes are on the far right.

Project the olympic decatholon score on to the principal component:

```{r}
data("olympic", package = "ade4")
olympic$score
```

## 7.8 PCA as an exploratory tool: using extra information

```{r}
pcaMsig3 = dudi.pca(Msig3transp, center = TRUE, scale = TRUE,
                    scannf = FALSE, nf = 4)
#One principal component seems sufficient
fviz_screeplot(pcaMsig3) + ggtitle("")
ids = rownames(Msig3transp)
celltypes = factor(substr(ids, 7, 9))
status = factor(substr(ids, 1, 3))
table(celltypes)
cbind(pcaMsig3$li, tibble(Cluster = celltypes, sample = ids)) %>%
ggplot(aes(x = Axis1, y = Axis2)) +
  geom_point(aes(color = Cluster), size = 5) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_vline(xintercept = 0, linetype = 2) +
  scale_color_discrete(name = "Cluster") + coord_fixed()
```

## 7.8.1 Mass Spectroscopy Data Analysis

```{r}
library(xcms)
load(here("data","mat1xcms.RData")) 
dim(mat1)
pcamat1 = dudi.pca(t(mat1), scannf = FALSE, nf = 3)
fviz_eig(pcamat1, geom = "bar", bar_width = 0.7) + ggtitle("")
#use one principal component
dfmat1 = cbind(pcamat1$li, tibble(
    label = rownames(pcamat1$li),
    number = substr(label, 3, 4),
    type = factor(substr(label, 1, 2))))
pcsplot = ggplot(dfmat1,
  aes(x=Axis1, y=Axis2, label=label, group=number, colour=type)) +
 geom_text(size = 4, vjust = -0.5)+ geom_point(size = 3)+ylim(c(-18,19))
pcsplot + geom_hline(yintercept = 0, linetype = 2) +
  geom_vline(xintercept = 0, linetype = 2)
```

Knockouts are below their wildlife type, not random scattering.

```{r}
pcsplot + geom_line(colour = "red")
```

## 7.8.2 Biplots and scaling

```{r}
library("pheatmap")
load(here("data", "wine.RData"))
load(here("data", "wineClass.RData"))
wine[1:2, 1:7]
pheatmap(1 - cor(wine), treeheight_row = 0.2)
winePCAd = dudi.pca(wine, scannf=FALSE)
table(wine.class)
fviz_pca_biplot(winePCAd, geom = "point", habillage = wine.class,
   col.var = "violet", addEllipses = TRUE, ellipse.level = 0.69) +
   ggtitle("") + coord_fixed()
```

\alert{Hue and alcohol uncorrelated. Does this mean that we are looking at 90 degrees equals uncorrelated? What does 180 degrees mean?}

## 7.8.3 An example of weighted PCA

```{r}
data("x", package = "Hiiragi2013")
xwt = x[, x$genotype == "WT"]
sel = order(rowVars(Biobase::exprs(xwt)), decreasing = TRUE)[1:100]
xwt = xwt[sel, ]
tab = table(xwt$sampleGroup)
tab
xwt$weight = 1 / as.numeric(tab[xwt$sampleGroup])
pcaMouse = dudi.pca(as.data.frame(t(Biobase::exprs(xwt))),
  row.w = xwt$weight,
  center = TRUE, scale = TRUE, nf = 2, scannf = FALSE)
fviz_eig(pcaMouse) + ggtitle("")
fviz_pca_ind(pcaMouse, geom = "point", col.ind = xwt$sampleGroup) +
  ggtitle("") + coord_fixed()
```

# 7.11 Exercises

### exercise 7.1

```{r}
u=seq(2, 30 , by = 2)
v=seq(3, 12, by = 3)
X1 = u %*% t(v)
Materr = matrix(rnorm(60,1), nrow = 15, ncol = 4)
X = X1 + Materr
```
\alert{How am I supposed to visualize this?}

```{r}
ggpairs(as.data.frame(X))
```

### exercise 7.2

```{r}
library(rockchalk)
mu1 = 1; mu2 = 2; s1=2.5; s2=0.8; rho=0.9;
sigma = matrix(c(s1^2, s1*s2*rho, s1*s2*rho, s2^2),2)
sim2d = data.frame(mvrnorm(50, mu = c(mu1,mu2), Sigma = sigma))
ggplot(data.frame(sim2d),aes(x=X1,y=X2)) +
  geom_point()
```

```{r}
svdn<-svd(scale(sim2d))
svdn$d
ppdfn =tibble(PC1n =-svdn$u[, 1]*svdn$d[1],PC2n = svdn$u[, 2]*svdn$d[2])
#Plot those values, add points for x of pc with y=o, add points for y of pc with x=0
ggplot(ppdfn,aes(x = PC1n, y = PC2n))+ geom_point()+ xlab("PC1")+ ylab("PC2")+ geom_point(aes(x=PC1n,y=0),color="red") +
#Add segments of points to both rescaled 0 axes
geom_segment(aes(xend = PC1n, yend = 0), color = "red")+geom_hline(yintercept = 0, color = "purple", lwd=1.5, alpha=0.5)+xlim(-3.5, 2.7)+ ylim(-2,2)+ coord_fixed()

#Not rotated to 0
pc = as.matrix(sim2d) %*% svdn$v[, 1] %*% t(svdn$v[, 1])
bp = svdn$v[2, 1] / svdn$v[1, 1]
ap = mean(pc[, 2]) - bp * mean(pc[, 1])
ggplot(data.frame(sim2d),aes(x=X1,y=X2)) +
  geom_point(size = 2, shape = 21) + geom_segment(xend = pc[, 1], yend = pc[, 2]) +
  geom_abline(intercept = ap, slope = bp, col = "purple", lwd = 1.5) + coord_fixed()
```

### exercise 7.3

```{r}
svdn<-svd(scale(sim2d))
svdn$d
ppdfn =tibble(PC1n =-svdn$u[, 1]*svdn$d[1],PC2n = svdn$u[, 2]*svdn$d[2])
#Plot those values, add points for x of pc with y=o, add points for y of pc with x=0
ggplot(ppdfn,aes(x = PC1n, y = PC2n))+ geom_point()+ xlab("PC1")+ ylab("PC2")+ geom_point(aes(x=PC1n,y=0),color="red") +
#Add segments of points to both rescaled 0 axes
geom_segment(aes(xend = PC1n, yend = 0), color = "red")+geom_hline(yintercept = 0, color = "purple", lwd=1.5, alpha=0.5)+xlim(-3.5, 2.7)+ ylim(-2,2)

#Not rotated to 0
pc = as.matrix(sim2d) %*% svdn$v[, 1] %*% t(svdn$v[, 1])
bp = svdn$v[2, 1] / svdn$v[1, 1]
ap = mean(pc[, 2]) - bp * mean(pc[, 1])
ggplot(data.frame(sim2d),aes(x=X1,y=X2)) +
  geom_point(size = 2, shape = 21) + geom_segment(xend = pc[, 1], yend = pc[, 2]) +
  geom_abline(intercept = ap, slope = bp, col = "purple", lwd = 1.5) 
```

### exercise 7.4

```{r}
fviz_eig(pcaMouse, geom = "bar", bar_width = 0.3) + ggtitle("")
fviz_pca_var(pcaMouse, col.circle = "black") + ggtitle("")
fviz_pca_biplot(pcaMouse, geom = "point", habillage = xwt$sampleGroup, col.var = "violet", addEllipses = TRUE, ellipse.level = 0.69) + ggtitle("") + coord_fixed()
```

```{r, warning = "hide"}
#From Jill's
pca_unweighted = dudi.pca(as.data.frame(t(Biobase::exprs(xwt))),
  row.w = as.numeric(tab[xwt$sampleGroup]),
  center = TRUE, scale = TRUE, nf = 2, scannf = FALSE)

fviz_pca_var(pca_unweighted, col.circle = "black") + ggtitle("") + coord_fixed()
largePC1 <-pca_unweighted[,(pca_unweighted$co$Comp1 >= 30 | pca_unweighted$co$Comp1 <= -30)]

fviz_pca_biplot(largePC1, geom = "point", label = "var", 
                habillage = xwt$sampleGroup) +   
  ggtitle("") + 
  coord_fixed()
```
